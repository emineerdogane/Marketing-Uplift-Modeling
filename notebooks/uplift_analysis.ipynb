{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bfe363",
   "metadata": {},
   "source": [
    "# Uplift Modeling for Marketing Campaign Optimization\n",
    "\n",
    "A complete implementation of causal uplift modeling to identify which customers benefit most from marketing campaigns, enabling data-driven targeting strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5dd385",
   "metadata": {},
   "source": [
    "## Step 1: Upload the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d047fce",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Emine/OneDrive/Masa√ºst√º/Google Project/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Causal libraries (suppress duecredit warning)\n",
    "try:\n",
    "    from io import StringIO\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = StringIO()  # Temporarily suppress stderr\n",
    "    from causalml.inference.tree import UpliftRandomForestClassifier\n",
    "    sys.stderr = original_stderr  # Restore stderr\n",
    "    has_causalml = True\n",
    "except ImportError:\n",
    "    sys.stderr = original_stderr  # Restore stderr in case of import error\n",
    "    has_causalml = False\n",
    "    print('causalml not available, will use T-learner fallback')\n",
    "\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "print('‚úì Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb55ff",
   "metadata": {},
   "source": [
    "## Step 2: Load & Explore Starbucks Dataset\n",
    "\n",
    "First, load all three files and understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Starbucks data\n",
    "data_dir = r\"C:\\Users\\Emine\\OneDrive\\Masa√ºst√º\\Google Project\\Starbucks dataset\"\n",
    "\n",
    "print('Loading Starbucks data...')\n",
    "portfolio = pd.read_json(f'{data_dir}\\\\portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json(f'{data_dir}\\\\profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json(f'{data_dir}\\\\transcript.json', orient='records', lines=True)\n",
    "\n",
    "print(f'\\n‚úì Data loaded:')\n",
    "print(f'  Portfolio (offers): {portfolio.shape}')\n",
    "print(f'  Profile (customers): {profile.shape}')\n",
    "print(f'  Transcript (events): {transcript.shape}')\n",
    "\n",
    "# Examine portfolio structure\n",
    "print('\\n=== PORTFOLIO (Offer Types) ===')\n",
    "print(portfolio.head())\n",
    "print(f'\\nOffer types: {portfolio[\"offer_type\"].value_counts().to_dict()}')\n",
    "\n",
    "# Examine profile structure\n",
    "print('\\n=== PROFILE (Customer Demographics) ===')\n",
    "print(profile.head())\n",
    "print(f'\\nGender distribution: {profile[\"gender\"].value_counts().to_dict()}')\n",
    "print(f'Age stats: min={profile[\"age\"].min()}, max={profile[\"age\"].max()}, mean={profile[\"age\"].mean():.1f}')\n",
    "\n",
    "# Examine transcript structure\n",
    "print('\\n=== TRANSCRIPT (Event Log) ===')\n",
    "print(transcript.head(10))\n",
    "print(f'\\nEvent types: {transcript[\"event\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5690d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare data\n",
    "print('Cleaning data...')\n",
    "\n",
    "# 1. Clean profile (remove invalid/missing data)\n",
    "profile = profile[profile['age'] != 118].copy()  # Remove placeholder ages\n",
    "profile = profile.dropna(subset=['gender', 'income'])\n",
    "profile['became_member_on'] = pd.to_datetime(profile['became_member_on'], format='%Y%m%d')\n",
    "profile = profile.rename(columns={'id': 'person'})\n",
    "\n",
    "# 2. Parse transcript value column\n",
    "transcript['offer_id'] = transcript['value'].apply(\n",
    "    lambda x: x.get('offer_id') or x.get('offer id') if isinstance(x, dict) else None\n",
    ")\n",
    "transcript['amount'] = transcript['value'].apply(\n",
    "    lambda x: x.get('amount', 0) if isinstance(x, dict) else 0\n",
    ")\n",
    "\n",
    "# 3. Add offer details to transcript\n",
    "transcript = transcript.merge(\n",
    "    portfolio[['id', 'offer_type', 'difficulty', 'reward', 'duration']],\n",
    "    left_on='offer_id', right_on='id', how='left', suffixes=('', '_offer')\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì Cleaned profile: {profile.shape}')\n",
    "print(f'‚úì Enhanced transcript: {transcript.shape}')\n",
    "print(f'\\nEvent breakdown:')\n",
    "print(transcript['event'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f688da2",
   "metadata": {},
   "source": [
    "### Step 2.1: Feature Engineering - Customer Behavior\n",
    "\n",
    "Create features from transaction history (BEFORE any offer was sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate customer behavior features from transaction history\n",
    "# These are pre-treatment features (before offers)\n",
    "\n",
    "# Get transaction events only (they have 'amount' in value field)\n",
    "transactions = transcript[transcript['event'] == 'transaction'].copy()\n",
    "\n",
    "# Extract amount from value dictionary for transactions\n",
    "transactions['transaction_amount'] = transactions['value'].apply(\n",
    "    lambda x: x.get('amount', 0) if isinstance(x, dict) else 0\n",
    ")\n",
    "\n",
    "# Customer-level transaction features\n",
    "customer_features = transactions.groupby('person').agg({\n",
    "    'transaction_amount': ['sum', 'mean', 'count', 'std'],\n",
    "    'time': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "customer_features.columns = ['person', 'total_spent', 'avg_transaction', \n",
    "                               'transaction_count', 'spending_std', \n",
    "                               'first_transaction_time', 'last_transaction_time']\n",
    "\n",
    "# Fill missing std with 0 (customers with only 1 transaction)\n",
    "customer_features['spending_std'] = customer_features['spending_std'].fillna(0)\n",
    "\n",
    "# Calculate recency and frequency\n",
    "customer_features['transaction_range'] = (\n",
    "    customer_features['last_transaction_time'] - customer_features['first_transaction_time']\n",
    ")\n",
    "customer_features['transaction_frequency'] = np.where(\n",
    "    customer_features['transaction_range'] > 0,\n",
    "    customer_features['transaction_count'] / (customer_features['transaction_range'] / 24),  # per day\n",
    "    0\n",
    ")\n",
    "\n",
    "print('Customer behavior features:')\n",
    "print(customer_features.head())\n",
    "print(f'\\nFeatures created for {len(customer_features):,} customers who made transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d994e8",
   "metadata": {},
   "source": [
    "### Step 2.2: Define Treatment & Control Groups\n",
    "\n",
    "Carefully select ONE specific offer to analyze (to ensure clean causal inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ONE specific offer for clean analysis\n",
    "# Strategy: Pick the most common BOGO offer\n",
    "\n",
    "# Ensure offer_id exists (in case previous cells weren't run)\n",
    "if 'offer_id' not in transcript.columns:\n",
    "    transcript['offer_id'] = transcript['value'].apply(\n",
    "        lambda x: x.get('offer_id') or x.get('offer id') if isinstance(x, dict) else None\n",
    "    )\n",
    "\n",
    "bogo_offers = portfolio[portfolio['offer_type'] == 'bogo'].copy()\n",
    "print('Available BOGO offers:')\n",
    "print(bogo_offers[['id', 'difficulty', 'reward', 'duration']])\n",
    "\n",
    "# Count how many people received each BOGO offer\n",
    "offer_counts = transcript[\n",
    "    (transcript['event'] == 'offer received') & \n",
    "    (transcript['offer_id'].isin(bogo_offers['id']))\n",
    "]['offer_id'].value_counts()\n",
    "\n",
    "print(f'\\nOffer distribution:')\n",
    "print(offer_counts)\n",
    "\n",
    "# Select the most common one\n",
    "target_offer_id = offer_counts.index[0]\n",
    "target_offer_details = bogo_offers[bogo_offers['id'] == target_offer_id].iloc[0]\n",
    "\n",
    "print(f'\\n‚úì Selected offer: {target_offer_id}')\n",
    "print(f'  Difficulty: ${target_offer_details[\"difficulty\"]}')\n",
    "print(f'  Reward: ${target_offer_details[\"reward\"]}')\n",
    "print(f'  Duration: {target_offer_details[\"duration\"]} hours')\n",
    "print(f'  People who received it: {offer_counts.iloc[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define treatment and control groups - FIXED APPROACH\n",
    "# SOLUTION: Use broader outcome definition that works for both groups\n",
    "\n",
    "print('Creating uplift dataset...')\n",
    "\n",
    "# Get offer duration\n",
    "offer_duration = target_offer_details['duration']\n",
    "\n",
    "# Get all people who received this offer\n",
    "offer_received = transcript[\n",
    "    (transcript['event'] == 'offer received') & \n",
    "    (transcript['offer_id'] == target_offer_id)\n",
    "][['person', 'time']].copy()\n",
    "offer_received.columns = ['person', 'offer_received_time']\n",
    "\n",
    "# Define treatment group\n",
    "treatment_group = set(offer_received['person'])\n",
    "\n",
    "# BROADER OUTCOME: Made at least 1 purchase during the ENTIRE observation period\n",
    "# This ensures both treatment and control can have variation\n",
    "# Use the full time range in the dataset\n",
    "all_transactions = transcript[transcript['event'] == 'transaction'].copy()\n",
    "time_min = transcript['time'].min()\n",
    "time_max = transcript['time'].max()\n",
    "\n",
    "print(f'Observation period: {time_min:.0f} - {time_max:.0f} hours (entire dataset)')\n",
    "\n",
    "# For each customer, check if they made ANY purchase\n",
    "customers_with_purchases = set(all_transactions['person'])\n",
    "\n",
    "# Build dataset\n",
    "results = []\n",
    "for person_id in profile['person']:\n",
    "    is_treatment = person_id in treatment_group\n",
    "    made_purchase = person_id in customers_with_purchases\n",
    "    results.append({\n",
    "        'person': person_id,\n",
    "        'treatment': int(is_treatment),\n",
    "        'y': int(made_purchase)\n",
    "    })\n",
    "\n",
    "outcome_df = pd.DataFrame(results)\n",
    "\n",
    "# Merge with profile (keep ALL columns for feature engineering)\n",
    "df = profile[['person', 'age', 'income', 'gender', 'became_member_on']].merge(\n",
    "    outcome_df, on='person'\n",
    ")\n",
    "\n",
    "# Merge transaction features\n",
    "df = df.merge(customer_features, on='person', how='left')\n",
    "transaction_cols = ['total_spent', 'avg_transaction', 'transaction_count', \n",
    "                     'spending_std', 'transaction_frequency']\n",
    "df[transaction_cols] = df[transaction_cols].fillna(0)\n",
    "\n",
    "print(f'\\n=== Dataset Summary ===')\n",
    "print(f'Total customers: {len(df):,}')\n",
    "print(f'Treatment: {df[\"treatment\"].sum():,} ({df[\"treatment\"].mean():.1%})')\n",
    "print(f'Control: {(1-df[\"treatment\"]).sum():,} ({(1-df[\"treatment\"]).mean():.1%})')\n",
    "print(f'\\nOutcome distribution:')\n",
    "print(f'  Treatment - y=1: {df[df[\"treatment\"]==1][\"y\"].sum():,} ({df[df[\"treatment\"]==1][\"y\"].mean():.2%})')\n",
    "print(f'  Treatment - y=0: {(df[df[\"treatment\"]==1][\"y\"]==0).sum():,}')\n",
    "print(f'  Control - y=1: {df[df[\"treatment\"]==0][\"y\"].sum():,} ({df[df[\"treatment\"]==0][\"y\"].mean():.2%})')\n",
    "print(f'  Control - y=0: {(df[df[\"treatment\"]==0][\"y\"]==0).sum():,}')\n",
    "print(f'\\n‚úì Naive Uplift: {df[df[\"treatment\"]==1][\"y\"].mean() - df[df[\"treatment\"]==0][\"y\"].mean():.4f}')\n",
    "\n",
    "# Critical check - MUST have both classes in both groups\n",
    "ctrl_classes = df[df[\"treatment\"]==0][\"y\"].nunique()\n",
    "treat_classes = df[df[\"treatment\"]==1][\"y\"].nunique()\n",
    "\n",
    "if ctrl_classes < 2 or treat_classes < 2:\n",
    "    print(f'\\n‚ö†Ô∏è  WARNING: Insufficient class variation!')\n",
    "    print(f'   Control has {ctrl_classes} unique classes')\n",
    "    print(f'   Treatment has {treat_classes} unique classes')\n",
    "    print(f'   This will cause model training to fail.')\n",
    "else:\n",
    "    print(f'\\n‚úì Class balance OK - both groups have 2 classes')\n",
    "    print(f'   Control: {ctrl_classes} classes, Treatment: {treat_classes} classes')\n",
    "\n",
    "print(f'\\n‚úì Ready for feature engineering (Step 2.4)')\n",
    "print(f'   Current columns: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f84335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check what's happening with the data\n",
    "print('=== DIAGNOSTIC ===')\n",
    "print(f'Total unique customers in transcript: {transcript[\"person\"].nunique()}')\n",
    "print(f'Customers who made transactions: {transcript[transcript[\"event\"]==\"transaction\"][\"person\"].nunique()}')\n",
    "\n",
    "# Calculate offer_received for diagnostic\n",
    "offer_received_diag = transcript[\n",
    "    (transcript['event'] == 'offer received') & \n",
    "    (transcript['offer_id'] == target_offer_id)\n",
    "][['person', 'time']].copy()\n",
    "\n",
    "print(f'Customers who received target offer: {len(offer_received_diag)}')\n",
    "\n",
    "# Check transaction timing\n",
    "all_transactions = transcript[transcript['event'] == 'transaction'].copy()\n",
    "print(f'\\nTransaction time range: {all_transactions[\"time\"].min():.0f} - {all_transactions[\"time\"].max():.0f}')\n",
    "print(f'Offer received time range: {offer_received_diag[\"time\"].min():.0f} - {offer_received_diag[\"time\"].max():.0f}')\n",
    "\n",
    "# Check how many control customers made ANY transaction\n",
    "treatment_group_diag = set(offer_received_diag['person'])\n",
    "control_customers = set(profile['person']) - treatment_group_diag\n",
    "control_with_transactions = set(all_transactions['person']) & control_customers\n",
    "print(f'\\nControl customers: {len(control_customers)}')\n",
    "print(f'Control customers with ANY transaction: {len(control_with_transactions)} ({len(control_with_transactions)/len(control_customers):.1%})')\n",
    "\n",
    "# Key insight\n",
    "print(f'\\nüí° KEY INSIGHT:')\n",
    "print(f'   If {len(control_with_transactions)/len(control_customers):.1%} of control customers made transactions,')\n",
    "print(f'   then our outcome should have variation in both groups!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2296f9",
   "metadata": {},
   "source": [
    "### Step 2.3: Final Feature Engineering\n",
    "\n",
    "Create all features for the uplift model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print('Engineering features...\\n')\n",
    "\n",
    "# 1. Membership features\n",
    "reference_date = pd.to_datetime('2018-07-26')\n",
    "df['membership_days'] = (reference_date - df['became_member_on']).dt.days\n",
    "df['member_tenure_years'] = df['membership_days'] / 365.25\n",
    "\n",
    "# 2. Demographics\n",
    "# Encode gender\n",
    "df = pd.get_dummies(df, columns=['gender'], prefix='gender', drop_first=False)\n",
    "\n",
    "# Age bins\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 70, 120], labels=['young', 'middle', 'senior', 'elderly'])\n",
    "df = pd.get_dummies(df, columns=['age_group'], prefix='age', drop_first=True)\n",
    "\n",
    "# Income bins  \n",
    "df['income_group'] = pd.qcut(df['income'], q=4, labels=['low', 'medium', 'high', 'very_high'], duplicates='drop')\n",
    "df = pd.get_dummies(df, columns=['income_group'], prefix='inc', drop_first=True)\n",
    "\n",
    "# 3. Interaction features\n",
    "df['age_income_interaction'] = df['age'] * df['income'] / 100000\n",
    "df['spend_per_membership_day'] = df['total_spent'] / (df['membership_days'] + 1)\n",
    "df['avg_transaction_x_frequency'] = df['avg_transaction'] * df['transaction_frequency']\n",
    "\n",
    "# 4. Behavioral segments\n",
    "df['is_active'] = (df['transaction_count'] > df['transaction_count'].median()).astype(int)\n",
    "df['is_high_value'] = (df['total_spent'] > df['total_spent'].quantile(0.75)).astype(int)\n",
    "df['is_frequent'] = (df['transaction_frequency'] > df['transaction_frequency'].median()).astype(int)\n",
    "\n",
    "# Select final features for modeling\n",
    "feature_list = [\n",
    "    # Demographics\n",
    "    'age', 'income', 'membership_days', 'member_tenure_years',\n",
    "    \n",
    "    # Gender (one-hot encoded)\n",
    "    'gender_F', 'gender_M', 'gender_O',\n",
    "    \n",
    "    # Age groups\n",
    "    'age_middle', 'age_senior', 'age_elderly',\n",
    "    \n",
    "    # Income groups  \n",
    "    'inc_medium', 'inc_high', 'inc_very_high',\n",
    "    \n",
    "    # Transaction behavior\n",
    "    'total_spent', 'avg_transaction', 'transaction_count',\n",
    "    'spending_std', 'transaction_frequency',\n",
    "    \n",
    "    # Derived features\n",
    "    'age_income_interaction', 'spend_per_membership_day',\n",
    "    'avg_transaction_x_frequency',\n",
    "    \n",
    "    # Segments\n",
    "    'is_active', 'is_high_value', 'is_frequent'\n",
    "]\n",
    "\n",
    "# Keep only features that exist in df\n",
    "feature_list = [f for f in feature_list if f in df.columns]\n",
    "\n",
    "# Map to x0, x1, x2... format for compatibility with rest of code\n",
    "for i, feat in enumerate(feature_list):\n",
    "    df[f'x{i}'] = df[feat]\n",
    "\n",
    "# Final dataset\n",
    "keep_cols = ['person', 'treatment', 'y'] + [f'x{i}' for i in range(len(feature_list))]\n",
    "df = df[keep_cols].dropna()\n",
    "\n",
    "print(f'=== Final Uplift Modeling Dataset ===')\n",
    "print(f'Total observations: {len(df):,}')\n",
    "print(f'Features: {len(feature_list)}')\n",
    "print(f'Treatment rate: {df[\"treatment\"].mean():.2%}')\n",
    "print(f'Overall conversion: {df[\"y\"].mean():.2%}')\n",
    "print(f'\\nTreatment effect (naive):')\n",
    "print(f'  Control conversion: {df[df[\"treatment\"]==0][\"y\"].mean():.2%}')\n",
    "print(f'  Treatment conversion: {df[df[\"treatment\"]==1][\"y\"].mean():.2%}')\n",
    "print(f'  Absolute lift: {df[df[\"treatment\"]==1][\"y\"].mean() - df[df[\"treatment\"]==0][\"y\"].mean():.2%}')\n",
    "\n",
    "print(f'\\n‚úì Feature mapping:')\n",
    "for i, feat in enumerate(feature_list[:10]):  # Show first 10\n",
    "    print(f'  x{i}: {feat}')\n",
    "if len(feature_list) > 10:\n",
    "    print(f'  ... and {len(feature_list)-10} more features')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8c182",
   "metadata": {},
   "source": [
    "## Step 3: EDA - Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size:', df.shape)\n",
    "print('\\nTreatment distribution:')\n",
    "print(df['treatment'].value_counts(normalize=True))\n",
    "print('\\nOverall conversion rate:', df['y'].mean())\n",
    "print('\\nConversion by treatment group:')\n",
    "print(df.groupby('treatment')['y'].mean())\n",
    "\n",
    "# Chart: conversion by treatment\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='treatment', y='y', data=df, estimator='mean', errorbar=None)\n",
    "plt.title('Conversion Rate by Treatment')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5da25a",
   "metadata": {},
   "source": [
    "## Step 4: Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c.startswith('x')]\n",
    "X = df[features].values\n",
    "t = df['treatment'].values\n",
    "y = df['y'].values\n",
    "\n",
    "X_train, X_test, t_train, t_test, y_train, y_test = train_test_split(\n",
    "    X, t, y, test_size=0.3, random_state=42, stratify=t\n",
    ")\n",
    "print(f'Train size: {len(X_train)}, Test size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01285d",
   "metadata": {},
   "source": [
    "## Step 5: T-learner (baseline uplift model)\n",
    "\n",
    "We train separate models for control and treatment, calculating uplift = y1_hat - y0_hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control model (t=0)\n",
    "model_ctrl = GradientBoostingClassifier(random_state=42)\n",
    "model_ctrl.fit(X_train[t_train==0], y_train[t_train==0])\n",
    "\n",
    "# Treatment model (t=1)\n",
    "model_tr = GradientBoostingClassifier(random_state=42)\n",
    "model_tr.fit(X_train[t_train==1], y_train[t_train==1])\n",
    "\n",
    "# Predict probabilities\n",
    "y0_hat = model_ctrl.predict_proba(X_test)[:,1]\n",
    "y1_hat = model_tr.predict_proba(X_test)[:,1]\n",
    "uplift_pred = y1_hat - y0_hat\n",
    "\n",
    "print('T-learner trained!')\n",
    "print(f'Uplift statistics: mean={uplift_pred.mean():.4f}, std={uplift_pred.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7f70f",
   "metadata": {},
   "source": [
    "## Step 6: (Optional) CausalML Uplift Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ac229",
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift_causalml = None\n",
    "if has_causalml:\n",
    "    try:\n",
    "        # Convert treatment to string for CausalML compatibility\n",
    "        t_train_str = t_train.astype(str)\n",
    "        \n",
    "        clf = UpliftRandomForestClassifier(n_estimators=200, control_name='0', random_state=42)\n",
    "        clf.fit(X=X_train, treatment=t_train_str, y=y_train)\n",
    "        uplift_causalml = clf.predict(X_test)\n",
    "        print(f'CausalML uplift mean: {uplift_causalml.mean():.4f}')\n",
    "    except Exception as e:\n",
    "        print('CausalML error:', e)\n",
    "else:\n",
    "    print('CausalML not installed, using T-learner.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420689cf",
   "metadata": {},
   "source": [
    "## Step 7: Uplift curve and AUUC (simple calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_curve(y_true, treatment, uplift_scores):\n",
    "    d = pd.DataFrame({'y': y_true, 't': treatment, 'uplift': uplift_scores})\n",
    "    d = d.sort_values('uplift', ascending=False).reset_index(drop=True)\n",
    "    d['cum_treated'] = (d['y'] * (d['t'] == 1)).cumsum()\n",
    "    d['cum_control'] = (d['y'] * (d['t'] == 0)).cumsum()\n",
    "    d['n_treated'] = (d['t'] == 1).cumsum()\n",
    "    d['n_control'] = (d['t'] == 0).cumsum()\n",
    "    d['rate_treated'] = d['cum_treated'] / d['n_treated'].replace(0, np.nan)\n",
    "    d['rate_control'] = d['cum_control'] / d['n_control'].replace(0, np.nan)\n",
    "    d['incremental'] = (d['rate_treated'] - d['rate_control']).fillna(0)\n",
    "    x = np.arange(1, len(d) + 1) / len(d)\n",
    "    y = d['incremental'].values\n",
    "    auuc = np.trapezoid(y, x)\n",
    "    return auuc, d\n",
    "\n",
    "auuc_val, curve_df = uplift_curve(y_test, t_test, uplift_pred)\n",
    "print(f'T-learner AUUC (approx): {auuc_val:.4f}')\n",
    "\n",
    "# Grafik: uplift curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(curve_df.index / len(curve_df), curve_df['incremental'])\n",
    "plt.xlabel('Fraction of population targeted')\n",
    "plt.ylabel('Incremental conversion rate')\n",
    "plt.title('Uplift Curve (T-learner)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672992bf",
   "metadata": {},
   "source": [
    "### Uplift Score Distribution\n",
    "\n",
    "Visualize the distribution of uplift scores across all customers to understand the spread and identify key segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uplift score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(uplift_pred, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Baseline (No Effect)')\n",
    "plt.axvline(x=uplift_pred.mean(), color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Mean Uplift ({uplift_pred.mean():.4f})')\n",
    "\n",
    "plt.xlabel('Uplift Score (Treatment Effect)', fontsize=12)\n",
    "plt.ylabel('Number of Customers', fontsize=12)\n",
    "plt.title('Distribution of Predicted Uplift Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics box\n",
    "stats_text = f'''Statistics:\n",
    "Mean: {uplift_pred.mean():.4f}\n",
    "Median: {np.median(uplift_pred):.4f}\n",
    "Std: {uplift_pred.std():.4f}\n",
    "Min: {uplift_pred.min():.4f}\n",
    "Max: {uplift_pred.max():.4f}'''\n",
    "\n",
    "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "         fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Uplift Distribution Summary:')\n",
    "print(f'   Positive uplift (treatment helps): {(uplift_pred > 0).sum()} customers ({(uplift_pred > 0).mean():.1%})')\n",
    "print(f'   Negative uplift (treatment hurts): {(uplift_pred < 0).sum()} customers ({(uplift_pred < 0).mean():.1%})')\n",
    "print(f'   High uplift (>0.1): {(uplift_pred > 0.1).sum()} customers ({(uplift_pred > 0.1).mean():.1%})')\n",
    "print(f'   Very high uplift (>0.2): {(uplift_pred > 0.2).sum()} customers ({(uplift_pred > 0.2).mean():.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f2902",
   "metadata": {},
   "source": [
    "## Step 8: 4 Quadrant Segmentation\n",
    "\n",
    "- **Persuadables:** High uplift, low y0 (converts only with treatment)\n",
    "- **Sure Things:** High y0 (will convert anyway, no need to spend money)\n",
    "- **Lost Causes:** Low y0 and y1 (will never convert)\n",
    "- **Sleeping Dogs:** Negative uplift (treatment hurts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9899b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold values (adjusted according to business)\n",
    "uplift_q75 = np.percentile(uplift_pred, 75)\n",
    "y0_q75 = np.percentile(y0_hat, 75)\n",
    "\n",
    "labels = []\n",
    "for u, y0i, y1i in zip(uplift_pred, y0_hat, y1_hat):\n",
    "    if (u >= uplift_q75) and (y0i < 0.2):\n",
    "        labels.append('Persuadable')\n",
    "    elif (y0i >= y0_q75):\n",
    "        labels.append('SureThing')\n",
    "    elif (y0i < 0.05) and (y1i < 0.05):\n",
    "        labels.append('LostCause')\n",
    "    elif (u < -0.05):\n",
    "        labels.append('SleepingDog')\n",
    "    else:\n",
    "        labels.append('Other')\n",
    "\n",
    "qdf = pd.DataFrame({\n",
    "    'y': y_test, 't': t_test, 'y0_hat': y0_hat, 'y1_hat': y1_hat,\n",
    "    'uplift': uplift_pred, 'label': labels\n",
    "})\n",
    "\n",
    "print('Quadrant distribution:')\n",
    "print(qdf['label'].value_counts(normalize=True))\n",
    "\n",
    "# Chart\n",
    "plt.figure(figsize=(8,5))\n",
    "qdf['label'].value_counts().plot(kind='bar')\n",
    "plt.title('4 Quadrant Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "qdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e77ea9",
   "metadata": {},
   "source": [
    "## Step 9: Business Impact & Resume Bullet Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b32cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_sure = (qdf['label'] == 'SureThing').mean()\n",
    "pct_persuadable = (qdf['label'] == 'Persuadable').mean()\n",
    "\n",
    "# Sample campaign numbers (should be replaced with real data)\n",
    "monthly_targeted_users = 200000\n",
    "avg_cpc = 0.25  # cost per click/impression\n",
    "\n",
    "estimated_sure = pct_sure * monthly_targeted_users\n",
    "estimated_monthly_savings = estimated_sure * avg_cpc\n",
    "\n",
    "print(f'SureThing rate (test): {pct_sure:.2%}')\n",
    "print(f'Persuadable rate (test): {pct_persuadable:.2%}')\n",
    "print(f'Monthly SureThing users: {int(estimated_sure):,}')\n",
    "print(f'Estimated monthly savings: ${estimated_monthly_savings:,.0f}\\n')\n",
    "\n",
    "resume_bullet = (\n",
    "    f\"Developed an Uplift Model using CausalML/T-learner to isolate the true incremental impact of marketing campaigns; \"\n",
    "    f\"identified {pct_sure:.1%} of users as 'Sure Things' to exclude from targeting, theoretically saving \"\n",
    "    f\"${estimated_monthly_savings:,.0f}/month in ad spend efficiency.\"\n",
    ")\n",
    "\n",
    "print('\\n=== RESUME BULLET (copy-paste) ===')\n",
    "print(resume_bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a49f0",
   "metadata": {},
   "source": [
    "## Step 10: Next Steps & Production Recommendations\n",
    "\n",
    "1. **Validation:** Validate uplift predictions with holdout A/B test\n",
    "2. **SHAP:** Add SHAP values for model explainability\n",
    "3. **Cost-aware targeting:** Develop targeting strategy that maximizes ROI\n",
    "4. **Production:** Model versioning with MLflow, automatic retrain pipeline with Airflow\n",
    "5. **Monitor:** Distribution shift and KPI tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b9a83",
   "metadata": {},
   "source": [
    "### 10.1: Validation - Holdout A/B Test\n",
    "\n",
    "Simulate validation by comparing predicted uplift vs actual uplift in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7558754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual uplift for validation\n",
    "actual_uplift = []\n",
    "for i in range(len(y_test)):\n",
    "    if t_test[i] == 1:  # Treatment group\n",
    "        actual_uplift.append(y_test[i])\n",
    "    else:  # Control group\n",
    "        actual_uplift.append(-y_test[i])\n",
    "\n",
    "# Bin users by predicted uplift\n",
    "n_bins = 10\n",
    "qdf['uplift_decile'] = pd.qcut(qdf['uplift'], q=n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Calculate actual uplift per decile\n",
    "validation_df = qdf.groupby('uplift_decile').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'avg_predicted_uplift': x['uplift'].mean(),\n",
    "        'avg_y_treatment': x[x['t'] == 1]['y'].mean(),\n",
    "        'avg_y_control': x[x['t'] == 0]['y'].mean(),\n",
    "        'actual_uplift': x[x['t'] == 1]['y'].mean() - x[x['t'] == 0]['y'].mean(),\n",
    "        'count': len(x)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print('Validation: Predicted vs Actual Uplift by Decile')\n",
    "print(validation_df)\n",
    "\n",
    "# Plot validation curve\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(validation_df['avg_predicted_uplift'], validation_df['actual_uplift'], s=100)\n",
    "plt.plot([-0.1, 0.3], [-0.1, 0.3], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('Predicted Uplift')\n",
    "plt.ylabel('Actual Uplift')\n",
    "plt.title('Uplift Model Validation: Predicted vs Actual')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "corr = validation_df['avg_predicted_uplift'].corr(validation_df['actual_uplift'])\n",
    "print(f'\\nCorrelation between predicted and actual uplift: {corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336aa82a",
   "metadata": {},
   "source": [
    "### 10.2: SHAP - Model Explainability\n",
    "\n",
    "Add SHAP values to understand which features drive uplift predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install shap if needed: pip install shap\n",
    "try:\n",
    "    import shap\n",
    "    has_shap = True\n",
    "except:\n",
    "    has_shap = False\n",
    "    print('SHAP not installed. Run: pip install shap')\n",
    "\n",
    "if has_shap:\n",
    "    # SHAP for treatment model (shows what drives high conversion in treatment group)\n",
    "    explainer_tr = shap.TreeExplainer(model_tr)\n",
    "    shap_values_tr = explainer_tr.shap_values(X_test)\n",
    "    \n",
    "    # For binary classification, take positive class\n",
    "    if isinstance(shap_values_tr, list):\n",
    "        shap_values_tr = shap_values_tr[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    shap.summary_plot(shap_values_tr, X_test, feature_names=features, show=False)\n",
    "    plt.title('SHAP Feature Importance - Treatment Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance for uplift (difference between treatment and control)\n",
    "    explainer_ctrl = shap.TreeExplainer(model_ctrl)\n",
    "    shap_values_ctrl = explainer_ctrl.shap_values(X_test)\n",
    "    \n",
    "    if isinstance(shap_values_ctrl, list):\n",
    "        shap_values_ctrl = shap_values_ctrl[1]\n",
    "    \n",
    "    # Uplift SHAP = SHAP(treatment) - SHAP(control)\n",
    "    shap_uplift = shap_values_tr - shap_values_ctrl\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    shap.summary_plot(shap_uplift, X_test, feature_names=features, show=False)\n",
    "    plt.title('SHAP Feature Importance - Uplift (Treatment - Control)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Top features driving uplift:')\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': np.abs(shap_uplift).mean(axis=0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(feature_importance)\n",
    "else:\n",
    "    print('Install SHAP to see feature importance: pip install shap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4760d",
   "metadata": {},
   "source": [
    "### 10.3: Cost-Aware Targeting\n",
    "\n",
    "Develop targeting strategy that maximizes ROI based on uplift, conversion cost, and customer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-aware targeting: maximize ROI\n",
    "# Define business parameters\n",
    "treatment_cost = 0.25  # Cost per treatment (e.g., ad click)\n",
    "customer_value = 50.0  # Average value of a conversion\n",
    "\n",
    "# Calculate expected profit per user\n",
    "qdf['expected_profit_no_treatment'] = customer_value * qdf['y0_hat']\n",
    "qdf['expected_profit_with_treatment'] = customer_value * qdf['y1_hat'] - treatment_cost\n",
    "qdf['incremental_profit'] = qdf['expected_profit_with_treatment'] - qdf['expected_profit_no_treatment']\n",
    "\n",
    "# Targeting strategy: only target users with positive incremental profit\n",
    "qdf['should_target'] = qdf['incremental_profit'] > 0\n",
    "\n",
    "print('Targeting Strategy Results:')\n",
    "print(f\"Users to target: {qdf['should_target'].sum()} ({qdf['should_target'].mean():.1%})\")\n",
    "print(f\"Users to skip: {(~qdf['should_target']).sum()} ({(~qdf['should_target']).mean():.1%})\")\n",
    "\n",
    "# Calculate ROI\n",
    "total_cost_all = len(qdf) * treatment_cost\n",
    "total_revenue_all = customer_value * (qdf['y1_hat'].sum())\n",
    "roi_all = (total_revenue_all - total_cost_all) / total_cost_all\n",
    "\n",
    "targeted_users = qdf[qdf['should_target']]\n",
    "total_cost_targeted = len(targeted_users) * treatment_cost\n",
    "total_revenue_targeted = customer_value * (targeted_users['y1_hat'].sum()) + \\\n",
    "                         customer_value * (qdf[~qdf['should_target']]['y0_hat'].sum())\n",
    "roi_targeted = (total_revenue_targeted - total_cost_targeted) / total_cost_targeted\n",
    "\n",
    "print(f'\\nROI comparison:')\n",
    "print(f'ROI (target all): {roi_all:.2%}')\n",
    "print(f'ROI (targeted strategy): {roi_targeted:.2%}')\n",
    "print(f'ROI improvement: {roi_targeted - roi_all:.2%}')\n",
    "\n",
    "# Visualize incremental profit distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(qdf['incremental_profit'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Break-even')\n",
    "plt.xlabel('Incremental Profit per User ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Incremental Profit\\n(Green = Target, Red = Skip)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show targeting breakdown by quadrant\n",
    "print('\\nTargeting by quadrant:')\n",
    "print(qdf.groupby('label')['should_target'].value_counts().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea83044",
   "metadata": {},
   "source": [
    "### 10.4: Production - MLflow & Airflow Setup\n",
    "\n",
    "Guidelines for productionizing the uplift model with MLflow for versioning and Airflow for automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model with MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    has_mlflow = True\n",
    "except:\n",
    "    has_mlflow = False\n",
    "    print('MLflow not installed. Run: pip install mlflow')\n",
    "\n",
    "if has_mlflow:\n",
    "    # Set experiment\n",
    "    mlflow.set_experiment('uplift_modeling')\n",
    "    \n",
    "    with mlflow.start_run(run_name='t_learner_uplift'):\n",
    "        # Log parameters\n",
    "        mlflow.log_param('model_type', 'T-learner')\n",
    "        mlflow.log_param('base_model', 'GradientBoostingClassifier')\n",
    "        mlflow.log_param('n_samples', len(df))\n",
    "        mlflow.log_param('test_size', 0.3)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('auuc', auuc_val)\n",
    "        mlflow.log_metric('mean_uplift', uplift_pred.mean())\n",
    "        mlflow.log_metric('std_uplift', uplift_pred.std())\n",
    "        mlflow.log_metric('pct_persuadable', pct_persuadable)\n",
    "        mlflow.log_metric('pct_sure_thing', pct_sure)\n",
    "        \n",
    "        # Log models\n",
    "        mlflow.sklearn.log_model(model_ctrl, 'control_model')\n",
    "        mlflow.sklearn.log_model(model_tr, 'treatment_model')\n",
    "        \n",
    "        # Log artifacts\n",
    "        import pickle\n",
    "        with open('uplift_predictions.pkl', 'wb') as f:\n",
    "            pickle.dump({'uplift': uplift_pred, 'y0_hat': y0_hat, 'y1_hat': y1_hat}, f)\n",
    "        mlflow.log_artifact('uplift_predictions.pkl')\n",
    "        \n",
    "        print('‚úì Model logged to MLflow')\n",
    "        print(f'Run ID: {mlflow.active_run().info.run_id}')\n",
    "else:\n",
    "    print('Install MLflow: pip install mlflow')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('PRODUCTION SETUP GUIDE')\n",
    "print('='*60)\n",
    "print('''\n",
    "1. MLflow Setup:\n",
    "   - Install: pip install mlflow\n",
    "   - Start server: mlflow ui --port 5000\n",
    "   - Access: http://localhost:5000\n",
    "\n",
    "2. Airflow DAG Example:\n",
    "   \n",
    "   from airflow import DAG\n",
    "   from airflow.operators.python import PythonOperator\n",
    "   from datetime import datetime, timedelta\n",
    "   \n",
    "   def retrain_uplift_model():\n",
    "       # Load new data\n",
    "       # Train models\n",
    "       # Log to MLflow\n",
    "       # Deploy to production\n",
    "       pass\n",
    "   \n",
    "   dag = DAG(\n",
    "       'uplift_model_retrain',\n",
    "       schedule_interval='@weekly',\n",
    "       start_date=datetime(2025, 1, 1),\n",
    "       catchup=False\n",
    "   )\n",
    "   \n",
    "   retrain = PythonOperator(\n",
    "       task_id='retrain_uplift',\n",
    "       python_callable=retrain_uplift_model,\n",
    "       dag=dag\n",
    "   )\n",
    "\n",
    "3. Deployment:\n",
    "   - Save models to S3/Azure Blob\n",
    "   - Serve via REST API (Flask/FastAPI)\n",
    "   - Monitor with Prometheus/Grafana\n",
    "   - A/B test new model versions\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc41cd",
   "metadata": {},
   "source": [
    "### 10.5: Monitoring - Distribution Shift & KPI Tracking\n",
    "\n",
    "Monitor model performance over time and detect data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f95ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate monitoring: detect distribution shift between train and test\n",
    "from scipy import stats\n",
    "\n",
    "print('='*60)\n",
    "print('MONITORING DASHBOARD')\n",
    "print('='*60)\n",
    "\n",
    "# 1. Feature Distribution Shift (KS Test)\n",
    "print('\\n1. Feature Distribution Shift (Kolmogorov-Smirnov Test):')\n",
    "print('-' * 60)\n",
    "shift_detected = False\n",
    "for i, feature in enumerate(features):\n",
    "    ks_stat, p_value = stats.ks_2samp(X_train[:, i], X_test[:, i])\n",
    "    status = '‚ö†Ô∏è DRIFT' if p_value < 0.05 else '‚úì OK'\n",
    "    if p_value < 0.05:\n",
    "        shift_detected = True\n",
    "    print(f'{feature}: KS={ks_stat:.4f}, p-value={p_value:.4f} {status}')\n",
    "\n",
    "# 2. Target Distribution\n",
    "print('\\n2. Target Distribution:')\n",
    "print('-' * 60)\n",
    "train_conversion = y_train.mean()\n",
    "test_conversion = y_test.mean()\n",
    "print(f'Training conversion rate: {train_conversion:.4f}')\n",
    "print(f'Test conversion rate: {test_conversion:.4f}')\n",
    "print(f'Difference: {abs(train_conversion - test_conversion):.4f}')\n",
    "\n",
    "# 3. Treatment Distribution\n",
    "print('\\n3. Treatment Distribution:')\n",
    "print('-' * 60)\n",
    "train_treatment = t_train.mean()\n",
    "test_treatment = t_test.mean()\n",
    "print(f'Training treatment rate: {train_treatment:.4f}')\n",
    "print(f'Test treatment rate: {test_treatment:.4f}')\n",
    "\n",
    "# 4. Model Performance KPIs\n",
    "print('\\n4. Model Performance KPIs:')\n",
    "print('-' * 60)\n",
    "print(f'AUUC: {auuc_val:.4f}')\n",
    "print(f'Mean predicted uplift: {uplift_pred.mean():.4f}')\n",
    "print(f'Std predicted uplift: {uplift_pred.std():.4f}')\n",
    "print(f'Persuadable rate: {pct_persuadable:.2%}')\n",
    "print(f'Sure Thing rate: {pct_sure:.2%}')\n",
    "\n",
    "# 5. Business KPIs\n",
    "print('\\n5. Business Impact KPIs:')\n",
    "print('-' * 60)\n",
    "print(f'Estimated monthly savings: ${estimated_monthly_savings:,.0f}')\n",
    "print(f'Users to exclude from targeting: {int(estimated_sure):,}')\n",
    "print(f'Cost efficiency improvement: {pct_sure:.1%}')\n",
    "\n",
    "# 6. Alert System\n",
    "print('\\n6. Alerts:')\n",
    "print('-' * 60)\n",
    "if shift_detected:\n",
    "    print('‚ö†Ô∏è  ALERT: Feature distribution shift detected!')\n",
    "    print('   Action: Consider retraining the model')\n",
    "else:\n",
    "    print('‚úì No distribution shift detected')\n",
    "\n",
    "if auuc_val < 0.01:\n",
    "    print('‚ö†Ô∏è  ALERT: Low AUUC score')\n",
    "    print('   Action: Review model performance')\n",
    "else:\n",
    "    print(f'‚úì AUUC score is healthy ({auuc_val:.4f})')\n",
    "\n",
    "# Visualization: Feature distributions (first 8 features)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, len(features))):\n",
    "    feature = features[i]\n",
    "    # Always convert to float first to avoid type issues\n",
    "    train_data = np.asarray(X_train[:, i], dtype=np.float64)\n",
    "    test_data = np.asarray(X_test[:, i], dtype=np.float64)\n",
    "    \n",
    "    # Check if binary feature (only 2 unique values)\n",
    "    unique_vals = len(np.unique(np.concatenate([train_data, test_data])))\n",
    "    \n",
    "    if unique_vals <= 2:\n",
    "        # For binary features, show text instead of histogram\n",
    "        axes[i].text(0.5, 0.5, f'{feature}\\n(Binary: 0/1)', \n",
    "                    ha='center', va='center', fontsize=12, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        axes[i].set_xlim(0, 1)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "    else:\n",
    "        # For continuous features, plot histogram\n",
    "        axes[i].hist(train_data, bins=30, alpha=0.5, label='Train', density=True)\n",
    "        axes[i].hist(test_data, bins=30, alpha=0.5, label='Test', density=True)\n",
    "        axes[i].legend()\n",
    "    \n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distribution Monitoring (Train vs Test)', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('MONITORING RECOMMENDATIONS:')\n",
    "print('='*60)\n",
    "print('''\n",
    "1. Set up automated monitoring:\n",
    "   - Daily: Track conversion rates, uplift distribution\n",
    "   - Weekly: Run KS tests for feature drift\n",
    "   - Monthly: Retrain model and compare performance\n",
    "\n",
    "2. Define alert thresholds:\n",
    "   - KS test p-value < 0.05 ‚Üí Retrain alert\n",
    "   - AUUC drops > 20% ‚Üí Performance alert\n",
    "   - Conversion rate changes > 30% ‚Üí Business alert\n",
    "\n",
    "3. Dashboard tools:\n",
    "   - Grafana for real-time metrics\n",
    "   - Evidently AI for ML monitoring\n",
    "   - Custom dashboards with Plotly/Dash\n",
    "\n",
    "4. Logging:\n",
    "   - Log all predictions with timestamps\n",
    "   - Store actual outcomes for validation\n",
    "   - Track model versions in production\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
